import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Assume df is your DataFrame with the indicators and 'buy' column

# Features for the model
features = ['RSI', 'MACD', 'MACD_Signal', 'MACD_Hist', 'VAMP', 'Supertrend', '%K', '%D', 'ADX', 'OBV', 'Donchian_High', 'Donchian_Low']
X = df[features]
y = df['buy']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the XGBoost model
model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print("Classification Report:")
print(report)

# Print predicted probabilities for the first few examples in the test set
print("Predicted probabilities for the first few examples in the test set:")
print(y_pred_proba[:10])



import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

# Assume df is your DataFrame with the indicators and 'buy' column

# Features for the model
features = ['RSI', 'MACD', 'MACD_Signal', 'MACD_Hist', 'VAMP', 'Supertrend', '%K', '%D', 'ADX', 'OBV', 'Donchian_High', 'Donchian_Low']
X = df[features]
y = df['buy']

# Handle NaN values by forward filling
X.fillna(method='ffill', inplace=True)
X.fillna(method='bfill', inplace=True)

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Reshape input to be 3D [samples, timesteps, features] for LSTM
timesteps = 1  # You can experiment with different numbers of timesteps
X_scaled_reshaped = X_scaled.reshape((X_scaled.shape[0], timesteps, X_scaled.shape[1]))

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled_reshaped, y, test_size=0.2, random_state=42)

# Build the LSTM model
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(timesteps, X_scaled.shape[1])))
model.add(Dropout(0.2))
model.add(LSTM(50, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)

# Make predictions
y_pred_proba = model.predict(X_test).flatten()
y_pred = (y_pred_proba >= 0.5).astype(int)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print("Classification Report:")
print(report)

# Print predicted probabilities for the first few examples in the test set
print("Predicted probabilities for the first few examples in the test set:")
print(y_pred_proba[:10])