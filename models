import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Assume df is your DataFrame with the indicators and 'buy' column

# Features for the model
features = ['RSI', 'MACD', 'MACD_Signal', 'MACD_Hist', 'VAMP', 'Supertrend', '%K', '%D', 'ADX', 'OBV', 'Donchian_High', 'Donchian_Low']
X = df[features]
y = df['buy']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the XGBoost model
model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print("Classification Report:")
print(report)

# Print predicted probabilities for the first few examples in the test set
print("Predicted probabilities for the first few examples in the test set:")
print(y_pred_proba[:10])



import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

# Assume df is your DataFrame with the indicators and 'buy' column

# Features for the model
features = ['RSI', 'MACD', 'MACD_Signal', 'MACD_Hist', 'VAMP', 'Supertrend', '%K', '%D', 'ADX', 'OBV', 'Donchian_High', 'Donchian_Low']
X = df[features]
y = df['buy']

# Handle NaN values by forward filling
X.fillna(method='ffill', inplace=True)
X.fillna(method='bfill', inplace=True)

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Reshape input to be 3D [samples, timesteps, features] for LSTM
timesteps = 1  # You can experiment with different numbers of timesteps
X_scaled_reshaped = X_scaled.reshape((X_scaled.shape[0], timesteps, X_scaled.shape[1]))

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled_reshaped, y, test_size=0.2, random_state=42)

# Build the LSTM model
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(timesteps, X_scaled.shape[1])))
model.add(Dropout(0.2))
model.add(LSTM(50, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)

# Make predictions
y_pred_proba = model.predict(X_test).flatten()
y_pred = (y_pred_proba >= 0.5).astype(int)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print("Classification Report:")
print(report)

# Print predicted probabilities for the first few examples in the test set
print("Predicted probabilities for the first few examples in the test set:")
print(y_pred_proba[:10])

ðŸ˜Š


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# Assume df is your DataFrame with the indicators and 'buy' column

# Features for the model
features = ['RSI', 'MACD', 'MACD_Signal', 'MACD_Hist', 'VAMP', 'Supertrend', '%K', '%D', 'ADX', 'OBV', 'Donchian_High', 'Donchian_Low']
X = df[features]
y = df['buy']

# Handle NaN values by forward filling
X.fillna(method='ffill', inplace=True)
X.fillna(method='bfill', inplace=True)

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Convert to PyTorch tensors
X_tensor = torch.tensor(X_scaled, dtype=torch.float32)
y_tensor = torch.tensor(y.values, dtype=torch.float32)

# Reshape input to be 3D [samples, timesteps, features] for LSTM
timesteps = 1  # You can experiment with different numbers of timesteps
X_tensor_reshaped = X_tensor.view(X_tensor.shape[0], timesteps, X_tensor.shape[1])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_tensor_reshaped, y_tensor, test_size=0.2, random_state=42)

# Create DataLoader for batch processing
train_data = TensorDataset(X_train, y_train)
test_data = TensorDataset(X_test, y_test)
train_loader = DataLoader(train_data, batch_size=32, shuffle=True)
test_loader = DataLoader(test_data, batch_size=32, shuffle=False)

# Define the LSTM model
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.dropout = nn.Dropout(0.2)
        self.fc = nn.Linear(hidden_size, output_size)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.dropout(out[:, -1, :])
        out = self.fc(out)
        out = self.sigmoid(out)
        return out

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Initialize the model, criterion and optimizer
input_size = X_tensor.shape[1]
hidden_size = 50
num_layers = 2
output_size = 1
model = LSTMModel(input_size, hidden_size, num_layers, output_size).to(device)
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Train the model
num_epochs = 50
model.train()
for epoch in range(num_epochs):
    for inputs, targets in train_loader:
        inputs, targets = inputs.to(device), targets.to(device)
        outputs = model(inputs).squeeze()
        loss = criterion(outputs, targets)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# Evaluate the model
model.eval()
with torch.no_grad():
    y_pred_proba = model(X_test.to(device)).squeeze().cpu().numpy()
    y_pred = (y_pred_proba >= 0.5).astype(int)

# Calculate accuracy and classification report
accuracy = accuracy_score(y_test.numpy(), y_pred)
report = classification_report(y_test.numpy(), y_pred)

print(f"Accuracy: {accuracy}")
print("Classification Report:")
print(report)

# Print predicted probabilities for the first few examples in the test set
print("Predicted probabilities for the first few examples in the test set:")
print(y_pred_proba[:10])